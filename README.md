# text-conditioned-3d-generation

The page will maintain various algorithms on text-conditioned 3d content generation, from object, human to scene.

## 1. Object

### 1.1 Text-conditioned 3D Object Generation

**[2023-arXiv]** One-2-3-45: Any Single Image to 3D Mesh in 45
Seconds without Per-Shape Optimization, [[paper](https://arxiv.org/pdf/2306.16928.pdf)] [[project](https://one-2-3-45.github.io/)]

**[2023-arXiv]** Magic123: One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors, [[paper](https://arxiv.org/pdf/2306.17843.pdf)] [[project](https://github.com/guochengqian/Magic123)]

**[2023-arXiv]** Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior, [[paper](https://arxiv.org/pdf/2303.14184.pdf)] [[project](https://github.com/junshutang/Make-It-3D)]

**[2023-arXiv]** Sketch and Text Guided Diffusion Model for Colored Point Cloud Generation, [[paper](https://arxiv.org/pdf/2308.02874.pdf)]

**[2023-arXiv]** Shap-E: Generating Conditional 3D Implicit Functions, [[paper](https://arxiv.org/pdf/2305.02463.pdf)]

**[2023-arXiv]** MATLABER: Material-Aware Text-to-3D via LAtent BRDF auto-EncodeR, [[paper](https://arxiv.org/pdf/2308.09278.pdf)] [[project](https://sheldontsui.github.io/projects/Matlaber)]

**[2023-arXiv]** Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation, [[paper](https://arxiv.org/pdf/2303.13873.pdf)] [[project](https://fantasia3d.github.io/)]

**[2023-arXiv]** Magic3D: High-Resolution Text-to-3D Content Creation, [[paper](https://arxiv.org/pdf/2211.10440.pdf)] [[project](https://research.nvidia.com/labs/dir/magic3d/)]

**[2023-arXiv]** TAPS3D: Text-Guided 3D Textured Shape Generation from Pseudo Supervision, [[paper](https://arxiv.org/pdf/2303.13273.pdf)] [[project](https://github.com/plusmultiply/TAPS3D)]

**[2023-arXiv]** 3D-LDM: Neural Implicit 3D Shape Generation with Latent Diffusion Models, [[paper](https://arxiv.org/pdf/2212.00842.pdf)]

**[2023-arXiv]** Autodecoding Latent 3D Diffusion Models, [[paper](https://arxiv.org/pdf/2307.05445.pdf)] [[project](https://snap-research.github.io/3DVADER/)]

**[2023-arXiv]** SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation, [[paper](https://arxiv.org/pdf/2212.04493.pdf)] [[project](https://yccyenchicheng.github.io/SDFusion/)]

**[2023-arXiv]** Pushing the Limits of 3D Shape Generation at Scale, [[paper](https://arxiv.org/pdf/2306.11510.pdf)]

**[2023-arXiv]** 3DGen: Triplane Latent Diffusion for Textured Mesh Generation, [[paper](https://arxiv.org/pdf/2303.05371.pdf)]

**[2023-arXiv]** ATT3D: Amortized Text-to-3D Object Synthesis, [[paper](https://arxiv.org/pdf/2306.07349.pdf)] [[project](https://research.nvidia.com/labs/toronto-ai/ATT3D/)]

**[2023-arXiv]** HyperNeRFGAN: Hypernetwork approach to 3D NeRF GAN, [[paper](https://arxiv.org/pdf/2301.11631.pdf)]

**[2023-arXiv]** Points-to-3D: Bridging the Gap between Sparse Points and Shape-Controllable Text-to-3D Generation, [[paper](https://arxiv.org/pdf/2307.13908.pdf)]

**[2023-arXiv]** DreamTime: An Improved Optimization Strategy for Text-to-3D Content Creation, [[paper](https://arxiv.org/pdf/2306.12422.pdf)]

**[2023-arXiv]** HiFA: High-fidelity Text-to-3D with Advanced Diffusion Guidance, [[paper](https://arxiv.org/pdf/2305.18766.pdf)] [[project](https://hifa-team.github.io/HiFA-site/)]

**[2023-arXiv]** ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation, [[paper](https://arxiv.org/pdf/2305.16213.pdf)] [[project](https://ml.cs.tsinghua.edu.cn/prolificdreamer/)]

**[2023-arXiv]** TextMesh: Generation of Realistic 3D Meshes From Text Prompts, [[paper](https://arxiv.org/pdf/2304.12439.pdf)] [[project](https://fabi92.github.io/textmesh/)]

**[2023-arXiv]** Re-imagine the Negative Prompt Algorithm: Transform 2D Diffusion into 3D, alleviate Janus problem and Beyond, [[paper](https://arxiv.org/pdf/2304.04968.pdf)]

**[2023-arXiv]** Debiasing Scores and Prompts of 2D Diffusion for Robust Text-to-3D Generation, [[paper](https://arxiv.org/pdf/2303.15413.pdf)]

**[2023-arXiv]** Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D Generation, [[paper](https://arxiv.org/pdf/2303.07937.pdf)] [[project](https://github.com/KU-CVLAB/3DFuse)]

**[2023-arXiv]** DITTO-NeRF: Diffusion-based Iterative Text To Omni-directional 3D Model, [[paper](https://arxiv.org/pdf/2304.02827.pdf)] [[project](https://janeyeon.github.io/ditto-nerf)]

**[2023-arXiv]** Text-driven Visual Synthesis with Latent Diffusion Prior, [[paper](https://arxiv.org/pdf/2302.08510.pdf)] [[project](https://latent-diffusion-prior.github.io/)]

**[2022-arXiv]** Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures, [[paper](https://arxiv.org/pdf/2211.07600.pdf)]

**[2022-arXiv]** DreamFusion: Text-to-3D using 2D Diffusion, [[paper](https://arxiv.org/pdf/2209.14988.pdf)] [[project](https://github.com/ashawkey/stable-dreamfusion)]

**[2022-arXiv]** Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D Generation, [[paper](https://arxiv.org/pdf/2212.00774.pdf)] [[project](https://pals.ttic.edu/p/score-jacobian-chaining)]

**[2022-arXiv]** Understanding Pure CLIP Guidance for Voxel Grid NeRF Models, [[paper](https://arxiv.org/pdf/2209.15172.pdf)] [[project](https://github.com/hanhung/PureCLIPNeRF)]

**[2021-arXiv]** Zero-Shot Text-Guided Object Generation with Dream Fields, [[paper](https://arxiv.org/pdf/2112.01455.pdf)] [[project](https://ajayj.com/dreamfields)]

**[2023-arXiv]** Deceptive-NeRF: Enhancing NeRF Reconstruction
using Pseudo-Observations from Diffusion Models, [[paper](https://arxiv.org/pdf/2305.15171.pdf)]

**[2023-AAAI]** 3D-TOGO: Towards Text-Guided Cross-Category 3D Object Generation, [[paper](https://arxiv.org/pdf/2212.01103.pdf)]

**[2023-CVPR]** Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and Text-to-Image Diffusion Models, [[paper](https://arxiv.org/pdf/2212.14704.pdf)] [[project](https://bluestyle97.github.io/dream3d/)]

**[2023-arXiv]** SALAD: Part-Level Latent Diffusion for 3D Shape Generation and Manipulation, [[paper](https://arxiv.org/pdf/2303.12236.pdf)] [[project](https://github.com/KAIST-Geometric-AI-Group/SALAD)]

**[2022-arXiv]** PointÂ·E: A System for Generating 3D Point Clouds from Complex Prompts, [[paper](https://arxiv.org/pdf/2212.08751.pdf)] [[project](https://github.com/openai/point-e)]

**[2022-arXiv]** LION: Latent Point Diffusion Models for 3D Shape Generation, [[paper](https://arxiv.org/pdf/2210.06978.pdf)]

**[2022-arXiv]** Fast Point Cloud Generation with Straight Flows, [[paper](https://arxiv.org/pdf/2212.01747.pdf)]

**[2023-arXiv]** Learning Versatile 3D Shape Generation with Improved AR Models, [[paper](https://arxiv.org/pdf/2303.14700.pdf)]

**[2023-arXiv]** Zero3D: Semantic-Driven 3D Shape Generation For Zero-shot Learning, [[paper](https://arxiv.org/pdf/2301.13591.pdf)]

**[2023-ICLR]** MeshDiffusion: Score-based Generative 3D Mesh Modeling, [[paper](https://arxiv.org/pdf/2303.08133.pdf)] [[project](https://github.com/lzzcd001/MeshDiffusion/)]

**[2023-arXiv]** ISS++: Image as Stepping Stone for Text-Guided 3D Shape Generation, [[paper](https://arxiv.org/pdf/2303.15181.pdf)] [[project](https://liuzhengzhe.github.io/ISS.github.io/)]

**[2022-arXiv]** ISS: Image as Stetting Stone for Text-Guided 3D Shape Generation, [[paper](https://arxiv.org/pdf/2209.04145.pdf)] [[project](https://liuzhengzhe.github.io/ISS.github.io/)]

**[2023-arXiv]** CLIP-Mesh: Generating textured meshes from text using pretrained image-text models, [[paper](https://arxiv.org/pdf/2203.13333.pdf)] [[project](https://github.com/NasirKhalid24/CLIP-Mesh)]

**[2018-arXiv]** Y2Seq2Seq: Cross-Modal Representation Learning for 3D Shape and Text by Joint Reconstruction and Prediction of View and Word Sequences, [[paper](https://arxiv.org/pdf/1811.02745.pdf)]

**[2023-arXiv]** T2TD: Text-3D Generation Model based on Prior Knowledge Guidance, [[paper](https://arxiv.org/pdf/2305.15753.pdf)]

**[2023-arXiv]** ZeroForge: Feedforward Text-to-Shape Without 3D Supervision, [[paper](https://arxiv.org/pdf/2306.08183.pdf)] [[project](https://github.com/Km3888/ZeroForge)]

**[2022-arXiv]** Diffusion-SDF: Text-to-Shape via Voxelized Diffusion, [[paper](https://arxiv.org/pdf/2212.03293.pdf)]

**[2022-arXiv]** ShapeCrafter: A Recursive Text-Conditioned 3D Shape Generation Model, [[paper](https://arxiv.org/pdf/2207.09446.pdf)]

**[2022-arXiv]** CLIP-Sculptor: Zero-Shot Generation of High-Fidelity and Diverse Shapes from Natural Language, [[paper](https://arxiv.org/pdf/2211.01427.pdf)] [[project](https://ivl.cs.brown.edu/#/projects/clip-sculptor)]

**[2021-arXiv]** CLIP-Forge: Towards Zero-Shot Text-to-Shape Generation, [[paper](https://arxiv.org/pdf/2110.02624.pdf)] [[project](https://github.com/AutodeskAILab/Clip-Forge)]

**[2022-arXiv]** Towards Implicit Text-Guided 3D Shape Generation, [[paper](https://arxiv.org/pdf/2203.14622.pdf)] [[project](https://github.com/liuzhengzhe/Towards-Implicit-Text-Guided-Shape-Generation)]

**[2019-arXiv]** Generation High resolution 3D model from natural language by Generative Adversarial Network, [[paper](https://arxiv.org/pdf/1901.07165.pdf)]

**[2018-arXiv]** Text2Shape: Generating Shapes from Natural
Language by Learning Joint Embeddings, [[paper](https://arxiv.org/pdf/1803.08495.pdf)] [[project](https://github.com/kchen92/text2shape/)]

**[2023-arXiv]** 3DShape2VecSet: A 3D Shape Representation for Neural Fields and Generative Diffusion Models, [[paper](https://arxiv.org/pdf/2301.11445.pdf)] [[project](https://github.com/1zb/3DShape2VecSet)]

**[2023-arXiv]** Michelangelo: Conditional 3D Shape Generation based on Shape-Image-Text Aligned Latent Representation, [[paper](https://arxiv.org/pdf/2306.17115.pdf)] [[project](https://github.com/NeuralCarver/michelangelo)]

**[2023-arXiv]** Joint Representation Learning for Text and 3D Point Cloud, [[paper](https://arxiv.org/pdf/2301.07584.pdf)]

**[2023-arXiv]** CLIP2: Contrastive Language-Image-Point Pretraining from Real-World Point Cloud Data, [[paper](https://arxiv.org/pdf/2303.12417.pdf)]

**[2023-arXiv]** ULIP-2: Towards Scalable Multimodal Pre-training For 3D Understanding, [[paper](https://arxiv.org/pdf/2305.08275.pdf)] [[project](https://github.com/salesforce/ULIP)]

**[2022-arXiv]** ULIP: Learning Unified Representation of Language, Image and Point Cloud for 3D Understanding, [[paper](https://arxiv.org/pdf/2212.05171.pdf)] [[project](https://github.com/salesforce/ULIP)]

**[2022-arXiv]** Neural Shape Compiler: A Unified Framework for Transforming between Text, Point Cloud, and Program, [[paper](https://arxiv.org/pdf/2212.12952.pdf)]

**[2023-arXiv]** Parts2Words: Learning Joint Embedding of Point Clouds and Texts by Bidirectional Matching between Parts and Words, [[paper](https://arxiv.org/pdf/2107.01872.pdf)]

### 1.2 Text-conditioned 3D Object Editing

**[2023-arXiv]** InstructP2P: Learning to Edit 3D Point Clouds with Text Instructions, [[paper](https://arxiv.org/pdf/2306.07154.pdf)]

**[2023-arXiv]** FocalDreamer: Text-driven 3D Editing via Focal-fusion Assembly, [[paper](https://arxiv.org/pdf/2308.10608.pdf)] [[project](https://focaldreamer.github.io/)]

**[2023-arXiv]** Blending-NeRF: Text-Driven Localized Editing in Neural Radiance Fields, [[paper](https://arxiv.org/pdf/2308.11974.pdf)]

**[2023-arXiv]** TextDeformer: Geometry Manipulation using Text Guidance, [[paper](https://arxiv.org/pdf/2304.13348.pdf)]

**[2023-arXiv]** CLIPXPlore: Coupled CLIP and Shape Spaces for 3D Shape Exploration, [[paper](ttps://arxiv.org/pdf/2306.08226.pdf)]

**[2023-arXiv]** Vox-E: Text-guided Voxel Editing of 3D Objects, [[paper](https://arxiv.org/pdf/2303.12048.pdf)] [[project](https://tau-vailab.github.io/Vox-E/)]

**[2023-arXiv]** DreamBooth3D: Subject-Driven Text-to-3D Generation, [[paper](https://arxiv.org/pdf/2303.13508.pdf)] [[project](https://dreambooth3d.github.io/)]

**[2023-arXiv]** RePaint-NeRF: NeRF Editting via Semantic Masks and Diffusion Models, [[paper](https://arxiv.org/pdf/2306.05668.pdf)]

**[2023-arXiv]** Blended-NeRF: Zero-Shot Object Generation and Blending in Existing Neural Radiance Fields, [[paper](https://arxiv.org/pdf/2306.12760.pdf)] [[project](https://www.vision.huji.ac.il/blended-nerf/)]

**[2023-arXiv]** DreamEditor: Text-Driven 3D Scene Editing with Neural Fields, [[paper](https://arxiv.org/pdf/2306.13455.pdf)]

**[2023-arXiv]** SINE: Semantic-driven Image-based NeRF Editing
with Prior-guided Editing Field, [[paper](https://arxiv.org/pdf/2303.13277.pdf)] [[project](https://zju3dv.github.io/sine/)]

**[2023-arXiv]** SKED: Sketch-guided Text-based 3D Editing, [[paper](https://arxiv.org/pdf/2303.10735.pdf)]

**[2022-arXiv]** LADIS: Language Disentanglement for 3D Shape Editing, [[paper](https://arxiv.org/pdf/2212.05011.pdf)]

**[2023-arXiv]** MatFuse: Controllable Material Generation with Diffusion Models, [[paper](https://arxiv.org/pdf/2308.11408.pdf)]

**[2023-arXiv]** Texture Generation on 3D Meshes with Point-UV Diffusion, [[paper](https://arxiv.org/pdf/2308.10490.pdf)]

**[2023-arXiv]** Generating Parametric BRDFs from Natural Language Descriptions, [[paper](https://arxiv.org/pdf/2306.15679.pdf)]

**[2023-arXiv]** Text-guided High-definition Consistency Texture Model, [[paper](https://arxiv.org/pdf/2305.05901.pdf)]

**[2023-arXiv]** X-Mesh: Towards Fast and Accurate Text-driven 3D Stylization via Dynamic Textual Guidance, [[paper](https://arxiv.org/pdf/2303.15764.pdf)] [[project](https://xmu-xiaoma666.github.io/Projects/X-Mesh/)]

**[2023-arXiv]** Instruct 3D-to-3D: Text Instruction Guided 3D-to-3D conversion, [[paper](https://arxiv.org/pdf/2303.15780.pdf)] [[project](https://sony.github.io/Instruct3Dto3D-doc/)]

**[2023-arXiv]** Text2Tex: Text-driven Texture Synthesis via Diffusion Models, [[paper](https://arxiv.org/pdf/2303.11396.pdf)] [[project](https://daveredrum.github.io/Text2Tex/)]

**[2023-arXiv]** TEXTure: Text-Guided Texturing of 3D Shapes, [[paper](https://arxiv.org/pdf/2302.01721.pdf)] [[project](https://texturepaper.github.io/TEXTurePaper/)]

**[2022-arXiv]** 3DDesigner: Towards Photorealistic 3D Object Generation and Editing with Text-guided Diffusion Models, [[paper](https://arxiv.org/pdf/2211.14108.pdf)] [[project](https://3ddesigner-diffusion.github.io/)]

**[2022-arXiv]** TANGO: Text-driven Photorealistic and Robust 3D Stylization via Lighting Decomposition, [[paper](https://arxiv.org/pdf/2210.11277.pdf)] [[project](https://cyw-3d.github.io/tango/)]

**[2021-arXiv]** Text2Mesh: Text-Driven Neural Stylization for Meshes, [[paper](https://arxiv.org/pdf/2112.03221.pdf)] [[project](https://threedle.github.io/text2mesh/)]

**[2020-arXiv]** Convolutional Generation of Textured 3D Meshes, [[paper](https://arxiv.org/pdf/2006.07660.pdf)] [[project](https://github.com/dariopavllo/convmesh)]

## 2. Scene

### 2.1 Text-conditioned 3D Object Generation

**[2023-arXiv]** , [[paper]()] [[project]()]

**[2023-arXiv]** , [[paper]()] [[project]()]

**[2023-arXiv]** , [[paper]()] [[project]()]

### 2.2 Text-conditioned 3D Scene Editing

**[2023-arXiv]** , [[paper]()] [[project]()]

**[2023-arXiv]** , [[paper]()] [[project]()]

**[2023-arXiv]** , [[paper]()] [[project]()]

## 3. Human

### 3.1 Text-conditioned 3D Human Generation

**[2023-arXiv]** , [[paper]()] [[project]()]

**[2023-arXiv]** , [[paper]()] [[project]()]

**[2023-arXiv]** , [[paper]()] [[project]()]

### 3.2 Text-conditioned 3D Human Editing

**[2023-arXiv]** , [[paper]()] [[project]()]

**[2023-arXiv]** , [[paper]()] [[project]()]

**[2023-arXiv]** , [[paper]()] [[project]()]

<br>

## Experts

[Hao Su](http://cseweb.ucsd.edu/~haosu/)(UC San Diego): 3D Deep Learning

[Matthias NieÃner](https://www.niessnerlab.org/members/matthias_niessner/profile.html)(TUM): 3D reconstruction, Semantic 3D Scene Understanding

</br>
